{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AW7if5yKyKvc",
        "outputId": "4ce53014-f6fc-4106-ba43-d5b20612b6c7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "time: 0 ns (started: 2025-04-01 16:07:21 +07:00)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "%pip install -q ipython-autotime tenacity\n",
        "%load_ext autotime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IDgOcNvsx3t9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 1.45 s (started: 2025-04-01 16:07:21 +07:00)\n"
          ]
        }
      ],
      "source": [
        "import selenium.webdriver\n",
        "from selenium.webdriver.chrome.service import Service\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from tenacity import retry, stop_after_attempt, wait_fixed\n",
        "import concurrent.futures\n",
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "from datetime import datetime\n",
        "import json\n",
        "from pprint import pprint\n",
        "import pandas as pd\n",
        "import time\n",
        "from queue import Queue\n",
        "import threading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "eKdPEHfpwVKc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-04-01 16:07:22 +07:00)\n"
          ]
        }
      ],
      "source": [
        "def get_chrome_options():\n",
        "  chrome_options = Options()\n",
        "  chrome_options.add_argument(\"--headless=new\")\n",
        "  chrome_options.add_argument(\"--disable-gpu\")\n",
        "  chrome_options.add_argument(\"--no-sandbox\")\n",
        "  chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
        "  chrome_options.add_argument(\"--disable-extensions\")\n",
        "  chrome_options.add_argument(\"--disable-logging\")\n",
        "  chrome_options.add_argument(\"--log-level=3\")\n",
        "  chrome_options.add_argument(\"--disable-images\")\n",
        "  chrome_options.add_argument(\"--disable-notifications\")\n",
        "  chrome_options.add_argument(\"--disable-web-security\")\n",
        "  chrome_options.page_load_strategy = \"eager\"\n",
        "  return chrome_options\n",
        "\n",
        "drive_path = \"C:\\\\ChromeDriver\\\\chromedriver.exe\"\n",
        "def get_service():\n",
        "  service = Service(executable_path=drive_path)\n",
        "  return service"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ba43vjK-whAv"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-04-01 16:07:25 +07:00)\n"
          ]
        }
      ],
      "source": [
        "def get_article_urls(page_url):\n",
        "    response = requests.get(page_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    articles = soup.find_all('h3', class_='title-news') + soup.find_all('h2', class_='title-news')\n",
        "    urls = [a.find('a')['href'] for a in articles if a.find('a')]\n",
        "    print(f\"URL page: {page_url}, số url: {len(urls)}\")\n",
        "    return urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "mtKC3jLOwhlQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-04-01 16:07:28 +07:00)\n"
          ]
        }
      ],
      "source": [
        "# Lấy dữ liệu bài viết từ URL\n",
        "@retry(stop=stop_after_attempt(3), wait=wait_fixed(5))\n",
        "def get_article_data(url, driver):\n",
        "      try:\n",
        "          driver.get(url)\n",
        "          WebDriverWait(driver, 5).until(\n",
        "              EC.presence_of_element_located((By.TAG_NAME, \"body\"))\n",
        "          )\n",
        "          print(f\"Loaded {url}\")\n",
        "\n",
        "          soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
        "          data = {}\n",
        "\n",
        "          data['title'] = soup.find('h1', class_='title-detail').get_text(strip=True) if soup.find('h1', class_='title-detail') else None\n",
        "          data['description'] = soup.find('p', class_='description').get_text(strip=True) if soup.find('p', class_='description') else None\n",
        "          data['date'] = soup.find('span', class_='date').get_text(strip=True) if soup.find('span', class_='date') else None\n",
        "          breadcrumb = soup.find('ul', class_='breadcrumb')\n",
        "          if breadcrumb:\n",
        "              category_links = breadcrumb.find_all('a')\n",
        "              categories = [link.get_text(strip=True) for link in category_links if link]\n",
        "              # Remove  category if it is \"Công nghệ\"\n",
        "              categories = [cat for cat in categories if cat != 'Công nghệ']\n",
        "              # Remove duplicates\n",
        "              categories = list(dict.fromkeys(categories))\n",
        "              # Join with commas\n",
        "              data['category'] = ', '.join(categories) if categories else 'Khác'\n",
        "          else:\n",
        "              print('else')\n",
        "              data['category'] = soup.find('meta', itemprop='articleSection')['content'] if soup.find('meta', itemprop='articleSection') else 'Công nghệ'\n",
        "        \n",
        "          data['content'] = \"\\n\".join([p.get_text(strip=True) for p in soup.find_all('p', class_='Normal')]) or None\n",
        "          img_tag = soup.find('img', class_='lazy')\n",
        "          data['thumbnail'] = 'https:' + (img_tag.get('data-src') or img_tag.get('src')) if img_tag and not (img_tag.get('data-src') or img_tag.get('src')).startswith('http') else img_tag.get('data-src') or img_tag.get('src') if img_tag else None\n",
        "          author_tag = soup.find('span', class_='author_mail')\n",
        "          data['author'] = author_tag.get_text(strip=True) if author_tag else None\n",
        "          if not data['author']:\n",
        "            authors = soup.find('p', class_='Normal', style='text-align:right;')\n",
        "            # if !authors:\n",
        "            if not authors:\n",
        "                authors = soup.find('p', class_='Normal', style='align: right;')\n",
        "            if not authors:\n",
        "                authors = soup.find('p', class_='Normal')[-1].get_text(strip=True)\n",
        "            if authors:\n",
        "                authors = authors.find('strong').get_text(strip=True) if authors.find('strong') else authors.get_text(strip=True)\n",
        "                data['author'] = authors\n",
        "            else:\n",
        "                data['author'] = \"Không xác định\"\n",
        "          data['tags'] = soup.find('meta', attrs={'name': 'its_tag'})['content'].split(', ') if soup.find('meta', attrs={'name': 'its_tag'}) else []\n",
        "          data['url'] = url\n",
        "          data['group'] = 'Công nghệ'\n",
        "          total_comment_label = soup.find('label', id='total_comment')\n",
        "        #   print(f\"total_comment_label: {total_comment_label}\")\n",
        "          data['nums_of_comments'] = int(total_comment_label.get_text(strip=True)) if total_comment_label else 0\n",
        "\n",
        "          return data\n",
        "      except Exception as e:\n",
        "          print(f\"Error processing {url}: {e}\")\n",
        "          raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded https://vnexpress.net/cach-xac-dinh-tai-khoan-zalo-co-tin-nhan-an-4867741.html\n",
            "{'author': 'Hoài Anh - Trường Giang',\n",
            " 'category': 'Trải nghiệm, Tư vấn',\n",
            " 'content': 'Hoài Anh - Trường Giang',\n",
            " 'date': 'Thứ hai, 31/3/2025, 14:00 (GMT+7)',\n",
            " 'description': 'Zalo cho phép ẩn cuộc hội thoại bí mật, không thể xem ở giao '\n",
            "                'diện chính và cần mật khẩu để truy cập.',\n",
            " 'group': 'Công nghệ',\n",
            " 'nums_of_comments': 3,\n",
            " 'tags': ['zalo', 'tin nhắn ẩn', 'ứng dụng nhắn tin'],\n",
            " 'thumbnail': None,\n",
            " 'title': 'Cách xác định tài khoản Zalo có tin nhắn ẩn',\n",
            " 'url': 'https://vnexpress.net/cach-xac-dinh-tai-khoan-zalo-co-tin-nhan-an-4867741.html'}\n",
            "time: 6.92 s (started: 2025-04-01 16:07:31 +07:00)\n"
          ]
        }
      ],
      "source": [
        "# url = \"https://vnexpress.net/facebook-them-lua-chon-quay-ve-nguyen-ban-4866947.html\"\n",
        "url = \"https://vnexpress.net/cach-xac-dinh-tai-khoan-zalo-co-tin-nhan-an-4867741.html\"\n",
        "service = Service(executable_path=drive_path)\n",
        "chrome_options = get_chrome_options()\n",
        "driver = selenium.webdriver.Chrome(service=service, options=chrome_options)\n",
        "article_info = get_article_data(url, driver)\n",
        "pprint(article_info)\n",
        "driver.quit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pt952N753GNg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "time: 0 ns (started: 2025-04-01 16:07:40 +07:00)\n"
          ]
        }
      ],
      "source": [
        "def fetch_all_articles(unique_urls, max_workers=5):\n",
        "    queue = Queue()\n",
        "    for url in unique_urls:\n",
        "        queue.put(url)\n",
        "\n",
        "    results = []\n",
        "    failed_urls = []\n",
        "\n",
        "    def worker():\n",
        "        with selenium.webdriver.Chrome(service=get_service(), options=get_chrome_options()) as browser:\n",
        "            while not queue.empty():\n",
        "                try:\n",
        "                    url = queue.get_nowait()\n",
        "                except Queue.Empty:\n",
        "                    break\n",
        "                try:\n",
        "                    article_info = get_article_data(url, browser)\n",
        "                    if article_info:\n",
        "                        results.append(article_info)\n",
        "                    else:\n",
        "                        failed_urls.append(url)\n",
        "                except Exception as e:\n",
        "                    print(f\"Failed to process {url}: {e}\")\n",
        "                    failed_urls.append(url)\n",
        "                finally:\n",
        "                    queue.task_done()\n",
        "\n",
        "    threads = []\n",
        "    for _ in range(max_workers):\n",
        "        t = threading.Thread(target=worker)\n",
        "        t.start()\n",
        "        threads.append(t)\n",
        "\n",
        "    for t in threads:\n",
        "        t.join()\n",
        "\n",
        "    # # Retry failed URLs sequentially\n",
        "    # if failed_urls:\n",
        "    #     print(f\"Retrying {len(failed_urls)} failed URLs...\")\n",
        "    #     with selenium.webdriver.Chrome(service=get_service(), options=get_chrome_options()) as browser:\n",
        "    #         for url in failed_urls[:]:  # Copy list to modify during iteration\n",
        "    #             try:\n",
        "    #                 article_info = get_article_data(url, driver)\n",
        "    #                 if article_info:\n",
        "    #                     results.append(article_info)\n",
        "    #                     failed_urls.remove(url)\n",
        "    #             except Exception as e:\n",
        "    #                 print(f\"Retry failed for {url}: {e}\")\n",
        "\n",
        "    print(f\"Đã thu thập {len(results)} bài báo, thất bại {len(failed_urls)} URL\")\n",
        "    return results, failed_urls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3ppPX6p4J2C",
        "outputId": "478a98a8-114d-4c19-f551-88856a605629"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "URL page: https://vnexpress.net/cong-nghe-p1, số url: 43\n",
            "URL page: https://vnexpress.net/cong-nghe-p2, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p3, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p4, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p5, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p6, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p7, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p8, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p9, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p10, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p11, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p12, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p13, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p14, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p15, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p16, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p17, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p18, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p19, số url: 30\n",
            "URL page: https://vnexpress.net/cong-nghe-p20, số url: 30\n",
            "Tổng số URL duy nhất: 607\n",
            "time: 5.72 s (started: 2025-04-01 16:07:58 +07:00)\n"
          ]
        }
      ],
      "source": [
        "base_url = \"https://vnexpress.net/cong-nghe\"\n",
        "# base_url = \"https://vnexpress.net/khoa-hoc\"\n",
        "all_urls_page = []\n",
        "for page in range(1, 21):\n",
        "    page_url = f\"{base_url}-p{page}\"\n",
        "    article_urls = get_article_urls(page_url)\n",
        "    all_urls_page.extend(article_urls)\n",
        "\n",
        "unique_urls = list(set(all_urls_page))\n",
        "print(f\"Tổng số URL duy nhất: {len(unique_urls)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lấy dữ liệu bài viết từ các URL\n",
        "all_data, failed_urls = fetch_all_articles(unique_urls, max_workers=5)\n",
        "print(f\"Số bài báo thu thập được: {len(all_data)}\")\n",
        "print(f\"Số URL thất bại: {len(failed_urls)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcIgvWUT4ViU"
      },
      "outputs": [],
      "source": [
        "print(f\"Số bài báo thu thập được: {len(all_data)}\")\n",
        "print(f\"Số URL thất bại: {len(failed_urls)}\")\n",
        "from pprint import pprint\n",
        "pprint(all_data[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data_2, failed_urls_2 = fetch_all_articles(failed_urls, max_workers=5)\n",
        "print(f\"Số bài báo thu thập được: {len(all_data_2)}\")\n",
        "print(f\"Số URL thất bại: {len(failed_urls_2)}\")\n",
        "from pprint import pprint\n",
        "pprint(all_data_2[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_data.extend(all_data_2)\n",
        "print(f\"Số bài báo thu thập được: {len(all_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Tạo một list để lưu trữ các dòng dữ liệu\n",
        "rows = []\n",
        "\n",
        "# Lặp qua mỗi bài viết\n",
        "for article in all_data:\n",
        "  if article is not None and isinstance(article, dict):\n",
        "    title = article['title'] if article['title'] is not None else ''\n",
        "    description = article['description'] if article['description'] is not None else ''\n",
        "    date = article['date'] if article['date'] is not None else ''\n",
        "    category = article['category'] if article['category'] is not None else ''\n",
        "    thumbnail = article['thumbnail'] if article['thumbnail'] is not None else ''\n",
        "    content = article['content'] if article['content'] is not None else ''\n",
        "    author = article['author'] if article['author'] is not None else ''\n",
        "    tags = ', '.join(article['tags'])  if article['tags'] is not None else ''\n",
        "    group = article['group'] if article['group'] is not None else ''\n",
        "    nums_of_comments = article['nums_of_comments'] if article['nums_of_comments'] is not None else 0\n",
        "    url = article['url'] if article['url'] is not None else ''\n",
        "\n",
        "    # Thêm dòng dữ liệu vào list\n",
        "    rows.append({\n",
        "        'title': title,\n",
        "        'description': description,\n",
        "        'date': date,\n",
        "        'category': category,\n",
        "        'thumbnail': thumbnail,\n",
        "        'content': content,\n",
        "        'author': author,\n",
        "        'tags': tags,\n",
        "        'group': group,\n",
        "        'nums_of_comments': nums_of_comments,\n",
        "        'url': url,\n",
        "    })\n",
        "  else:\n",
        "    print(f\"Skipping invalid article: {article}\")\n",
        "\n",
        "# Tạo DataFrame từ list các dòng dữ liệu\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Lưu DataFrame thành file CSV\n",
        "df.to_csv('vnexpress_congnghe_raw_data.csv', index=False, encoding='utf-8-sig')\n",
        "\n",
        "print(\"DataFrame đã được lưu thành file all_data.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
